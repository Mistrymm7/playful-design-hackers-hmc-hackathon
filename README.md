# TEAM : AXON 

## Developed by PlAYFUL DESIGN HACKERS at HMC Hackathon 

## SUMMARY:
Bridging the gap between technologies like open-source libraries, web applications, and modeling software to create more advanced, multifunctional, and innovative frameworks was our initial idea. It was an interesting challenge to see how far we could go during these two days of the HMC-Hackathon 2021. We decided to use different open-source ML algorithms to train hand-drawings, emotions, and sounds for achieving different interactive results, from geometric shapes to physical reactions databases.
Let us pull the common grounds and evolve with the process is what made us bring our project where we intervene various levels in the workflow (from concept to management) and create constant feedback loops from the user to designer.

 

### 1.     Scribble: [Check out Web Interface](https://mistrymm7.github.io/playful-design-hackers-hmc-hackathon/sketch-to-geometry-web-interface-client/)
If we say you draw a basic sketch and our mock-up generates 3D geometry for you. Does it excite you? It should be a symbiotic process where the software or web tool is adaptive to the style of working of the designer and can also navigate the interface conveniently. In that context, our idea was to develop a scribbler or sketcher for designers who may not be tech-savvy or prefer hand-sketched designs. Workflow shouldn’t become a hindrance to their creative thoughts. The requirement from the user is to draw primitive sketches and customize the design according to the context using sliders or other input methods. Subsequently, with the use of web sockets our prototype feeds required actions into the rhino-grasshopper interface to generate the results. It doesn’t stop at geometry creation but also gives you a window to analyze your design like Daylight analysis, Radiation, etc.
![Sketch](/assets/sketch.gif)

### 2.     ML sound recognition: [Check out Web Interface](https://mistrymm7.github.io/playful-design-hackers-hmc-hackathon/speech-controlled-arduino-lighting-web-interface-client/)
If we say you speak and we manage various functions in the building for you, does it excite you? Interactive methods between the built environment and aged/disabled users with the purpose of independence in every private and public area. Therefore, we trained a machine learning model to receive a physical reaction that is responsive to sound. In our model, we made light fixtures to respond to our speech and execute functions like change color, functionality off-on, etc.
![Sound](/assets/speechcontrolled.gif)

### 3.     ML emotion recognition:
What connects building to us? Isn’t it emotions! Security cameras can be used to recognize your experiential emotions at a certain space of a building and we create a database that can be used as feedback to generate an evident-based design in further projects. In our mock-up, the first step was to use machine learning algorithms to recognize the facial emotions categorized as four different classes: "Sad", "Happy", "Angry", and "Surprised" and secondly, putting it as feedback from the building to generate the database. Further our thoughts were to change the color of light fixtures based on the emotional response of the user. This leads the environment to be multi-sensory, adaptive, and interactive. For instance, an angry mood corresponds to a function of a softer color tone.
![Emotion](/assets/emotion.gif)





![Team](/assets/team.jpeg)





